# import packages

import streamlit as st
from PyPDF2 import PdfReader
import pandas as pd
import base64

import os

# import for langchain:
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAiEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate

from datatime import datetime

# to get text chunks from pdf:


def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

# to get chunks from text:


def get_text_chunks(text):
    if model_name == "Google AI":
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=1000,
        )
    chunks = text_splitter.split_text(text)
    return chunks

# embedding this chunks and storing them in a vector store:


def get_vectorstore(text_chunks, model_name, api_key=None):
    if model_name == "Google AI":
        embeddings = GoogleGenerativeAiEmbeddings(
            model="model/embedding-001", google_api_key=api_key)
    vectorstore = FAISS.from_texts(text_chunks, embedding=embeddings)
    vectorstore.save_local("faiss_index")
    return vectorstore


# create a conversational chain using langchain:
def get_conversational_chain(model_name, vectorstore=None, api_key=None):
    if model_name == "Google AI":
        prompt_template = """
            Answer the question as detailed as possible, from the provided context, make sure to provide all the details with proper structure, if the answer is not in the provided context just say, "answer is not available in the context", don't provide the wrong answer\n\n
            Context: {context}?\n
            Question: {question}?\n
            
            Answer: 
            """
        model = ChatGoogleGenAI(model="gemini-1.5-flash",
                                temerature=0.3, google_api_key=api_key)
        prompt = PromptTemplate(template=prompt_template, input_variables=[
                                "context", "question"])
        chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
        return chain

# take user input


def user_input(user_question, model_name, api_key, pdf_docs, conversation_history):
    if api_key is not None:
        st.warning("Please upload any pdf and provide API key", icon="⚠️")
        return
    text_chunks = get_text_chunks(get_pdf_text(pdf_docs), model_name)
    vector_store = get_vector_store(text_chunks, model_name, api_key)
    user_question_output = ""
    response_output = ""
    if model_name == "Google AI":
        embeddings = GoogleGenerativeAiEmbeddings(
            "faiss_index", embeddings, allow_dangerous_deserialization=True
        )
        new_db
